{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from os.path import join, isdir, isfile\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training steps\n",
    "CROSS_START = 0\n",
    "CROSS_MAX = 10\n",
    "EPOCH_START = 0\n",
    "EPOCH_MAX = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Network parameters\n",
    "NUM_BATCH = 4\n",
    "patch_size = 512\n",
    "INPUT_CH = 2\n",
    "NUM_LABEL = 2\n",
    "TR_LR = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nifti_array_from_subject(_dict):\n",
    "    \n",
    "    nifti_FLAIR = nib.load(_dict['FLAIR']).get_data()\n",
    "    nifti_T1 = nib.load(_dict['T1']).get_data()\n",
    "    nifti_GT = nib.load(_dict['GT']).get_data()\n",
    "    nifti_GT = np.flip(nifti_GT.astype(np.int32), axis=0)\n",
    "\n",
    "    nifti_FLAIR = (nifti_FLAIR - nifti_FLAIR.mean()) / nifti_FLAIR.std()\n",
    "    nifti_T1 = (nifti_T1 - nifti_T1.mean()) / nifti_T1.std()    \n",
    "    \n",
    "    nifti_FLAIR = nifti_FLAIR[:,:,:]\n",
    "    nifti_T1 = nifti_T1[:,:,:]\n",
    "    nifti_GT = nifti_GT[:,:,:]\n",
    "                \n",
    "    # non-zero patch index & zero patch index\n",
    "    nz_temp = np.mean(nifti_GT, axis=(0,1))>0\n",
    "    nz_index = list(compress(xrange(len(nz_temp)), nz_temp))\n",
    "    z_index = list(compress(xrange(len(np.logical_not(nz_temp))), np.logical_not(nz_temp)))\n",
    "    \n",
    "    # zero patch index selection\n",
    "    z_index = random.sample(z_index, nz_index.__len__())\n",
    "    \n",
    "    # shuffle non-zero patch index and zero patch index\n",
    "    sel_index = nz_index + z_index\n",
    "    random.shuffle(sel_index)\n",
    "    \n",
    "    # output with selected patch\n",
    "    nifti_FLAIR = nifti_FLAIR[:,:,sel_index]\n",
    "    nifti_T1 = nifti_T1[:,:,sel_index]\n",
    "    nifti_GT = nifti_GT[:,:,sel_index]\n",
    "    nifti_SEG = nifti_SEG[:,:,sel_index]\n",
    "                \n",
    "    return nifti_FLAIR, nifti_T1, nifti_GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dir(_dir):\n",
    "    if not os.path.exists(_dir): os.makedirs(_dir)\n",
    "    return _dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data(batch_data):\n",
    "    \"\"\"\n",
    "    input: batch_data (t1+flair)\n",
    "    \n",
    "    down_n: (pool)-conv-conv\n",
    "    up_n: unpool-conv-conv\n",
    "    \n",
    "    batch_input [N,480,480,2]\n",
    "    down_1 [N,,,]\n",
    "    down_2 [N,,,]\n",
    "    down_3 [N,,,]\n",
    "    down_4 [N,,,]\n",
    "    down_5 [N,,,]\n",
    "    \n",
    "    up_1 [N,,,]\n",
    "    up_2 [N,,,]\n",
    "    up_3 [N,,,]\n",
    "    up_4 [N,,,]\n",
    "    \n",
    "    output\n",
    "    \n",
    "    output: output of the model\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('down_1'):\n",
    "\n",
    "        down_1 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=batch_data, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        down_1 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_1, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))        \n",
    "\n",
    "    with tf.variable_scope('down_2'):\n",
    "\n",
    "        down_2 = tf.layers.max_pooling2d( inputs=down_1,  pool_size=2, strides=2)\n",
    "        down_2 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_2, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        down_2 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_2, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))        \n",
    "\n",
    "    with tf.variable_scope('down_3'):\n",
    "\n",
    "        down_3 = tf.layers.max_pooling2d( inputs=down_2,  pool_size=2, strides=2)\n",
    "        down_3 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_3, filters=512, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        down_3 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_3, filters=512, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))        \n",
    "\n",
    "    with tf.variable_scope('down_4'):\n",
    "\n",
    "        down_4 = tf.layers.max_pooling2d( inputs=down_3,  pool_size=2, strides=2)\n",
    "        down_4 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_4, filters=1024, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        down_4 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_4, filters=1024, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))            \n",
    "\n",
    "    with tf.variable_scope('down_5'):\n",
    "\n",
    "        down_5 = tf.layers.max_pooling2d( inputs=down_4,  pool_size=2, strides=2)\n",
    "        down_5 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_5, filters=1024, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        down_5 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=down_5, filters=1024, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "\n",
    "\n",
    "    with tf.variable_scope('up_1'):\n",
    "\n",
    "        up_1 = tf.layers.conv2d_transpose(  inputs=down_5, filters=1024, kernel_size=2, strides=2, activation=tf.nn.relu)\n",
    "        up_1 = tf.concat([down_4, up_1], axis=-1)\n",
    "        up_1 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_1, filters=1024, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        up_1 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_1, filters=1024, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "\n",
    "    with tf.variable_scope('up_2'):\n",
    "\n",
    "        up_2 = tf.layers.conv2d_transpose(  inputs=up_1, filters=512, kernel_size=2, strides=2, activation=tf.nn.relu)\n",
    "        up_2 = tf.concat([down_3, up_2], axis=-1)\n",
    "        up_2 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_2, filters=512, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        up_2 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_2, filters=512, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "\n",
    "\n",
    "    with tf.variable_scope('up_3'):\n",
    "\n",
    "        up_3 = tf.layers.conv2d_transpose(  inputs=up_2, filters=256, kernel_size=2, strides=2, activation=tf.nn.relu)\n",
    "        up_3 = tf.concat([down_2, up_3], axis=-1)\n",
    "        up_3 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_3, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        up_3 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_3, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "\n",
    "\n",
    "    with tf.variable_scope('up_4'):\n",
    "\n",
    "        up_4 = tf.layers.conv2d_transpose(  inputs=up_3, filters=256, kernel_size=2, strides=2, activation=tf.nn.relu)\n",
    "        up_4 = tf.concat([down_1, up_4], axis=-1)\n",
    "        up_4 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_4, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "        up_4 = tf.layers.batch_normalization(  tf.layers.conv2d(  inputs=up_4, filters=256, kernel_size=3, activation=tf.nn.relu, use_bias=True, padding='same'))\n",
    "\n",
    "    with tf.variable_scope('model_data_output'):\n",
    "\n",
    "        batch_output = tf.layers.conv2d(  inputs=up_4, filters=2, kernel_size=1, activation=tf.nn.relu, use_bias=False, padding='same')\n",
    "        \n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_label(batch_label):\n",
    "    \"\"\"\n",
    "    input: batch_label (raw)\n",
    "    \n",
    "    output: batch_label (one_hot)\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('model_label_output'):\n",
    "\n",
    "        label_output = tf.squeeze(   tf.one_hot(  indices=batch_label, depth=2, on_value=1.0, off_value=0.0, axis=-1), [-2])\n",
    "        \n",
    "    return label_output       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = tf.placeholder(tf.float32, shape=[None, None, None, INPUT_CH])\n",
    "batch_label = tf.placeholder(tf.int32, shape=[None, None, None, 1])\n",
    "batch_rate = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dice_loss'):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        logits = model_data(batch_data)    \n",
    "        labels = model_label(batch_label)\n",
    "\n",
    "        batch_loss = tf.losses.softmax_cross_entropy(  onehot_labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        optimizer = tf.train.AdadeltaOptimizer(batch_rate).minimize(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver_model = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True    \n",
    "\n",
    "result_dir = set_dir('./data/')\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    for c_cross in xrange(CROSS_MAX):\n",
    "\n",
    "        cross_dir = set_dir(result_dir + str(c_cross) +'/')\n",
    "        log_dir = set_dir(cross_dir + 'log/')\n",
    "        \n",
    "        train_list = load_list(list_path=cross_dir+'train_list.txt')\n",
    "        valid_list = load_list(list_path=cross_dir+'valid_list.txt')\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(log_dir + 'train', sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(log_dir + 'valid')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        \"\"\" Stacking data \"\"\"\n",
    "        tr_total_data, tr_total_label = stack_data(train_list, channel=INPUT_CH, flag=0)\n",
    "        vl_total_data, vl_total_label = stack_data(valid_list, channel=INPUT_CH, flag=0)\n",
    "\n",
    "        rate_current = TR_LR\n",
    "\n",
    "        for c_epoch in xrange(EPOCH_MAX):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_total_losses = []\n",
    "            valid_total_losses = []\n",
    "\n",
    "            \"\"\" train \"\"\"\n",
    "            tr_batch_step_total = int(tr_total_data.shape[0]/NUM_BATCH)\n",
    "            print c_epoch, 'epochs, training...', rate_current\n",
    "            for c_step in xrange(tr_batch_step_total):       \n",
    "\n",
    "                tr_batch_data = tr_total_data[c_step*NUM_BATCH:(c_step+1)*NUM_BATCH,:,:,:]\n",
    "                tr_batch_label = tr_total_label[c_step*NUM_BATCH:(c_step+1)*NUM_BATCH,:,:]\n",
    "\n",
    "                tr_batch_loss, _ = sess.run([batch_loss, optimizer],\n",
    "                                            feed_dict={\n",
    "                                                batch_data:tr_batch_data,\n",
    "                                                batch_label:tr_batch_label,\n",
    "                                                batch_rate:rate_current})\n",
    "                train_total_losses.append(tr_batch_loss)\n",
    "\n",
    "            \"\"\" valid \"\"\"\n",
    "            vl_batch_step_total = int(vl_total_data.shape[0]/NUM_BATCH)                \n",
    "            print c_epoch, 'epochs, validating...'            \n",
    "            for c_step in xrange(vl_batch_step_total):\n",
    "\n",
    "                vl_batch_data = vl_total_data[c_step*NUM_BATCH:(c_step+1)*NUM_BATCH,:,:,:]\n",
    "                vl_batch_label = vl_total_label[c_step*NUM_BATCH:(c_step+1)*NUM_BATCH,:,:]\n",
    "\n",
    "                vl_batch_loss = sess.run([batch_loss],\n",
    "                                            feed_dict={\n",
    "                                                batch_data:vl_batch_data,\n",
    "                                                batch_label:vl_batch_label})\n",
    "                valid_total_losses.append(vl_batch_loss)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            train_total_loss = np.mean(train_total_losses)\n",
    "            train_summary = tf.Summary(value=[tf.Summary.Value(tag='train loss', simple_value=train_total_loss)])\n",
    "            train_writer.add_summary(train_summary, c_epoch)\n",
    "\n",
    "            valid_total_loss = np.mean(valid_total_losses)\n",
    "            valid_summary = tf.Summary(value=[tf.Summary.Value(tag='valid loss', simple_value=valid_total_loss)])\n",
    "            valid_writer.add_summary(valid_summary, c_epoch)\n",
    "\n",
    "            print c_epoch, 'epoch train_total_loss:', train_total_loss, 'epoch valid_total_loss:', valid_total_loss, (end_time - start_time), 'seconds', c_cross\n",
    "\n",
    "            if c_epoch % 25 == 24:\n",
    "                rate_current = 0.5 * rate_current\n",
    "                saver_path = saver_model.save(sess, cross_dir+'saver_model.ckpt', global_step=c_epoch)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
